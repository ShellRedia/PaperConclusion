### 1. Bootstrap your own latent: A new approach to self-supervised Learning

论文地址:https://arxiv.org/abs/2006.07733

代码:https://github.com/deepmind/deepmind-research/tree/master/byol

这篇论文的观察是说现在的网络越来越大，越来越深，那就需要更多的数据来进行训练，而且还可能有梯度消失，梯度爆炸的情况。~呃，那能不能不做这么多题就得到较好的结果呢？~ 现在版本是利用自监督学习或无监督的方法利用更少的数据学到更好的特征表示。
