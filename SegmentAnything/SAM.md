### 1. Segment Anything

论文链接:https://arxiv.org/abs/2304.02643

代码地址:https://github.com/facebookresearch/segment-anything

应用地址:https://segment-anything.com

这篇文章的目标是建立一个图像分割的即时基础模型，实现在多种数据集上对其进行预训练后解决下游的分割任务。

这项工作的成功取决于任务、模型和数据三个部分，因此有三个关于图像分割的部分需要解决:

    (1) 什么任务适用于 zero-shot generalization?
    (2) 对应的模型结构应该是怎样的?
    (3) 什么样的数据能够驱动这项任务和模型?

本文首先首先定义一个通用的提示的分割任务，可以提供一个强大的预训练目标并支持广泛的下游应用。此任务需要一个支持灵活提示的模型，并且可以在提示时实时输出分段掩码以允许交互使用。此外，一个多样化的、大规模的数据源也是必需的。

首先提及的是 __任务__ 方面，__基础模型(foundation models)__ 可以通过使用 __prompting技术__ 来完成对新数据集和任务的zero-shot和few-shot学习。

__模型__ 方面，作者将提出的模型称为 SAM，并满足三个方面的条件: (1) 图像编码器能给出有效的embedding; (2) embedding对应的prompt; 以及 (3) 将(1)(2)两个信息源在一个轻量级的masked decoder 中进行组合，用于预测分割mask。总体而言，SAM模型具备灵活的提示支持、实时计算mask以及对歧义的感知的能力。

然后作者提到在 __数据__ 方面，其数据集需要适当的数据引擎进行构建。数据引擎包括三个阶段：辅助手动、半自动和全自动。

    (1) 辅助手动: SAM辅助标注员进行标注mask，类似于经典的交互式分割设置。
    (2) 半自动: SAM可以通过提示模型可能的目标位置自动生成一部分对象的mask，标注员则专注于标注剩余的对象，从而增加mask的多样性。
    (3) 全自动: 标注员通过在前景点上使用规则网格来提示SAM，每张图像平均生成约100个高质量的mask。

接下来展开谈谈文章在各个方面的具体实现:

关于任务，其intuition是将自然语言处理（NLP）中的prompt的概念转化到分割领域，其中提示可以是前景/背景点集、粗略的框或mask、自由形式的文本，或者一般情况下的任何指示图像中需要进行分割的信息。

而可提示分割任务的目标是在给定任何提示的情况下返回一个有效的分割mask。所谓有效mask要求是在模棱两可的提示或是可能涉及多个对象的情况下，也应该对其中至少一个对象生成一个合理的mask(参考下图)。且这个任务可以用于预训练，并且这样的预训练的结果可以直接作用于下游任务，因此是zero-shot的，此外还提了一系列相关的分割任务（交互式分割、边缘检测、超像素化、对象提案生成、前景分割、语义分割、实例分割、全景分割等）。

![Fig01](kirillov2023segment/01.png)

SAM的结构如下图，由三部分组成: an image encoder, a flexible prompt encoder, and a fast mask decoder，且该模型建立在ViT的基础上，接下来分别看看这几个部分做了什么:

(1) image encoder: 使用经过MAE预训练的ViT，采用一些trick用来支持高分辨率图像的输入。

(2) prompt encoder: 考虑两类提示: 稀疏提示（点、框、文本）和密集提示（mask）。对于稀疏提示，使用位置编码来表示点和框，位置编码与每种提示类型的learned embedding相加，而对于自由文本，则使用CLIP的现成文本编码器；对于密集提示（mask），则使用卷积进行embedding，并与图像embedding进行逐元素求和。

(3) mask decoder: 将图像embedding、提示embedding和输出token高效地映射到一个mask。采用了Transformer decoder block 的修改版本，后接动态mask预测头。修改后的decoder block在两个方向上（提示到图像embedding和图像embedding到提示）使用自注意力和交叉注意力来更新所有embedding。在运行两个block之后，我们上采样图像embedding, 使用多层MLP将输出token映射到一个动态线性分类器，计算每个图像位置处的掩码前景概率。

![Fig02](kirillov2023segment/02.png)

关于歧义的解决，则是对模型进行了修改，使其能够为单个提示预测多个输出mask。观察发现，预测3个mask输出就足以解决大多数常见情况（嵌套mask通常最多三层: 整体、部分和子部分, 原文:whole, part, and subpart）。在训练过程中，仅反向传播掩码的最小损失，通过为每个mask预测置信度得分（即估计的IoU）后进行排名。

其他还有关于效率和Loss的一些叙述，运行时间约50ms, 训练采用的Loss为 focal loss 和 dice loss。

然后是介绍作者他们弄的数据集和数据引擎1.1B mask dataset, SA-1B，这里主要就讲了怎么去通过之前说的三个阶段来做数据的，总的来说是一些怎么去做mask的操作和指标上的细节问题。

而后是结果方面的一些讨论，首先是Zero-Shot迁移实验，作者考虑了五个任务，其中四个与训练SAM的可提示分割任务明显不同。这些实验评估了SAM在训练过程中未见过的数据集和任务上的表现，这些数据集可能包括在SA-1B不存在的图像分布（例如图中所示的underwater or ego-centric图像）。

![Fig03](kirillov2023segment/03.png)

具体而言则是提示SAM执行以下任务: (1) 边缘检测，(2) 分割所有物体即目标提案生成，(3) 分割检测到的物体即实例分割，以及(4) 作为概念验证，从自由文本中分割物体。这四个任务与SAM训练时的可提示分割任务明显不同并通过提示工程实现。

在评估方面，除了使用标准的mIoU指标外，还采用人工方式来进行语义判断，其中注释员根据从1（无意义）到10（像素完美）的评分来评估mask质量。当然具体的评估项目也就是第7节开始的小标题所述了，作者有关于此有详细的图标总结。总的来说，在分割上的性能挺好，在其他例如边缘检测的任务上就不如近来的其他先进模型。

![Fig04](kirillov2023segment/04.png)

![Fig05](kirillov2023segment/05.png)

![Fig06](kirillov2023segment/06.png)

随后是一些消融实验，用以说明所设计工作的意义:
![Fig07](kirillov2023segment/07.png)

最后总结怎是吹了一波提示分割以及预训练模型在泛化性上的好处，以及一些细节分割上的局限性。

    另：这里提一下 OCTA 在官方 Demo 上的表现。

![Fig01](kirillov2023segment_Demo/01.png)

![Fig02](kirillov2023segment_Demo/02.png)

![Fig03](kirillov2023segment_Demo/03.png)

![Fig04](kirillov2023segment_Demo/04.png)

可以看出如同综述和 __论文 2.__ 分析的一样，对于轮廓清晰的大型联通对象的分割效果才比较好（例如FAZ）；对于RV本身无论怎么调整作为提示的正负点，效果也还是不太好，尤其是微小的末端区域。

</br>

### 2. How Segment Anything Model (SAM) Boost Medical Image Segmentation?

这篇论文主要是探究SAM在医学图像分割领域的表现，其实还是有点偏综述性质的。前两章主要是提到SAM的大热，提到了其中的优势和局限性，以及本文所要在各个模态和种类的医学图像中利用SAM以图像分割为目的的一些分析。和 __论文 1.__ 所提到的一样，对SAM的基本概念进行介绍：

    Segment Anything Model（SAM）在大规模的SA-1B数据集上进行训练，该数据集包含了前所未有的图像和标注数量，使得模型具备了强大的零样本泛化能力。SAM使用基于ViT的图像编码器提取图像特征并计算图像embedding，使用提示编码器embedding提示并整合用户交互。然后，从两个编码器中提取的信息通过轻量级的mask解码器进行组合，基于图像embedding、提示embedding和输出token生成分割结果。

( ~~上面机翻润色的，大致理解一下~~ )

然后是各组件Image Encoder，Prompt Encoder以及Mask Decoder的介绍，由于 __论文 1.__ 已经提过还有图，这里就省略了。还有一点点对于数据引擎的介绍，这是为了快速进行标注工作而构建的，交互式的意思就是先机器自己标，人看到有不对的可以改，最后稳定了全部由人点点点提示机器完成。

第三章开始就是介绍SAM在医学领域的一些表现了，自然是有好有坏。设置条件包括评估了SAM在肿瘤分割、非肿瘤组织分割和细胞核分割等整张切片成像（WSI）上的表现，进行包括使用单个正点提示的SAM、使用20个点提示（其中10个为正点，10个为负点）的SAM，以及在每个单个实例对象上使用所有点/框的不同场景，结果表明SAM在 __大型连通对象(large connected object)__ 的分割性能才比较好。文章对以下任务进行了更为细致的分析：增强对比度计算机断层扫描(CECT)下的肝肿瘤分割；结肠镜图像中的息肉分割；脑MRI分割；腹部CT器官分割以及内窥镜手术器械分割等。最后给了一张Dice指标为标准的直方图来比较不同的场景方法，简而言之就是直接将SAM应用于医学图像分割时，其泛化能力有限，且在不同数据集和任务之间差异显著。

![Fig01](zhang2023segment/01.png)

第四章讲SAM针对于医学图像的改进，具体而言是如下几个方面：

    （1）针对医学图像的特有格式（如NII和DICOM）进行相应改进。
    （2）在医学数据集上对SAM的各组件参数进行微调，例如通过在目标数据集上微调SAM，模型在皮肤癌分割任务中的Dice系数从81.25%提高到88.79%。
    （3）利用SAM进行输入增强，使用分割先验图。

总结提到未来的发展可以从以下几个方面着手：将SAM从2D扩展到3D医学图像、降低医学图像分割的标注成本、探索除点和框提示之外的其他提示形式、以及将SAM与眼动跟踪技术相结合，设计协作的人机交互系统，使放射科医生能够通过直接观察感兴趣区域获取分割掩码。

第五章就是纯纯的总结时间了，其实就是把第四章的总结扩展了一下，这里不再重复了。